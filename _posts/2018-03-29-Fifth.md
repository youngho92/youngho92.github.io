---
layout: post
title: (Reinforcement Learning) Training the artificially intelligent Blackjack player

---
### 0. Introduction to Blackjack

<b> 0-0. Blackjack Objective </b><br>

The objective of the game is to beat the dealer in one of the following ways:

<ul>
  <li>Get 21 points on the player's first two cards (called a "blackjack" or "natural"), without a dealer blackjack.</li>
  <li>Reach a final score higher than the dealer without exceeding 21.</li>
  <li>Let the dealer draw additional cards until their hand exceeds 21.</li>
</ul>
<br>


<b> 0-1. Game Procedure </b><br>

<img src="/images/RL/blackjack.png" width="700">
<br>

1) Betting: Players decide how much to bet
2) Dealing: Each player and the dealer are dealt two cards. One of the dealer's cards is faced down.
3) Action: Each player can decide which action to take out of stand, hit, doubledown, and split.
4) Reward: Each player receives reward based on the outcome.
<br>


<b> 0-2. Player's Action </b><br>

<ul>
  <li>Hit: Request another card. You can request a hit as many times as you like, but if your total goes over 21, you will Bust and therfore lose the hand. </li>
  <li>Stand: Request that you receive no more cards. Your current hand will be judged against the dealer's.</li>
  <li>Doubledown: If you select this option, you will get exactly one more card, your turn will end, and your bet will be doubled. Note: this option will normally only be available immediately after you receive your first two cards, however, Doubling after Splitting is usually allowed.</li>
  <li>Split: If you have two cards of the same denomination, you can split your cards into two hands and play each hand separately. Your original bet will be duplicated for the new hand. Note: according to Atlantic City Blackjack rules, you may only draw one additional card on each ace when splitting a pair of aces. Also note that this option will normally only be available immediately after you receive your first two cards.</li>
 </ul>
 <br>

<b> 0-3. Example </b><br>

<img src="/images/RL/procedure.png" width="700">

Here, the player stands after getting 19. The dealer reveals the hidden card and received one more card. The dealer wins in this game because the sum of dealer's cards is bigger than the player's.



### 1. Infinite-deck Blackjack

First, I trained the player assuming that there are infinite number of decks. Under this assumption, each card is drawn with equal probability.


<b>1-0. Basic Strategy </b><br>

There is already publicly known strategy for Blackjack game. The following shows how the basic strategy works under the specific rules: Re-splitting is allowed, Doubledown is allowed after splitting, Dealer hits on a soft 17.
<br>

<img src="/images/RL/basic.png" width="350">

Here, the rows show what cards are drawn on the player's side and the columns indicate the dealer's faced-up card. When the dealer's card is lower than 6 and the sum of the player's cards is higher than 12, it is always better for player to stand. In addition, the player is encouraged to hit when he or she has low cards. According to the simulation, I was able to achieve 45.2% of winning odds using this strategy.
<br>
<br>


<b>1-1. Q-Learning Strategy </b><br>

The following picture shows how reinforcement learning works.<br>
<img src="/images/RL/RL_concept.png" width="700">
<br>
As shown in the left, the baby learns how to walk based on the reward structure. If the baby in the picture falls down, he gets pain as a reward while he gets compliement if walking straight. In the same manner, I trained my model based on the outcome of millions of Blackjack games. I gave the positive reward to my model if it won and punished it when it lost. I trained the model as follow and we can see that the model gets better and better as the training number increases.

<img src="/images/RL/Q_learning.png" width="700">
<br>


<b>1-2. Q-Learning VS Basic Strategy </b><br>

I compared the Q-learning based model with the basic strategy in terms of winning odds. It turned out that the Q-learning strategy achieved the winning odds of 45.8%, which is slightly higher than the basic strategy.

<img src="/images/RL/Q_VS_Basic.png" width="700">
<br>
<br>

### 2. 6-Deck Blackjack

Second, I trained the player assuming that there are 6 sets of decks, which is the rule adopted by most casinos. In this case, cards drawn in the past will affect the future outcomes.


<b>2-0. Remaining Cards Matter </b><br>

<img src="/images/RL/Q_learning_formula.png" width="500">

This shows how I updated Q values based on states, actions, and rewards. I only considered cards on the table for the infinite-deck scenario because each card is drawn with equal probability, however, I had to take into account remaining cards for the 6-deck game. In order to consider remaining cards, I used the high-low card counting and the omega-II card counting.

<b>2-1. Card Counting Strategies</b><br>

<img src="/images/RL/card_counting.png" width="400">
I exploited the high-low card counting and the omega-II card counting. Whenever the new card is included in each category, I will add or subtract the corresponding number to count cards. The omega-II card counting system is similar to the other one, but is more complicated. The above tables show the model's results based on the high-low card counting for several scenarios.


<img src="/images/RL/hi_low_ex.png" width="700">
The left-hand side table indicates the optimal strategies when more low cards are remaining and the right one exhibits the opposite case. The model insists that the player has to stand more when there are more high cards remaining. This is reasonable because the probability of going bust increases if there are more high cards left.


<br>
<br>
### 3. Conclusion

I simulated 20000 Blackjack games with omega-II Q-learning, high-low Q-learning, double Q-learning, R-learning, and the basic strategy as follow.

<img src="/images/RL/result.png" width="700">

It turned out that the omega-II based Q-learning achieved winning odds of 53% which is higher than 48% of the basic strategy. I also trained the model with double Q-learning which compliments the overestimation of Q values of Q-learning. However, I was not able to find the signicant difference between the two models. R-learning, which uses the stochastic reward structure instead of the constant one, underperformed Q-learning in terms of winning rates.  


